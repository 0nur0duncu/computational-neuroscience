{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.v7labs.com/blog/neural-networks-activation-functions referans alınmıştır.\n",
    "<center>\n",
    "<h2>Sinir Ağı Aktivasyon Fonksiyonu nedir?</h2>\n",
    "</center>\n",
    "* Aktivasyon Fonksiyonu, bir nöronun aktive edilip edilmemesi gerektiğine karar verir. \n",
    "* Bu, daha basit matematiksel işlemler kullanarak nöronun ağa girdisinin tahmin sürecinde önemli olup olmadığına karar vereceği anlamına gelir.\n",
    "* Aktivasyon Fonksiyonunun rolü, bir düğüme (veya bir katmana) beslenen bir dizi girdi değerinden çıktı elde etmektir.\n",
    "\n",
    "🤔 Düğüm tam olarak nedir?\n",
    "\n",
    "👉 Sinir ağını beynimizle karşılaştırırsak, bir düğüm, bir dizi giriş sinyali - harici uyaran - alan bir nöronun kopyasıdır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d243db4b12f1c8f339e8cb_unnamed.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👉 Bu girdi sinyallerinin doğasına ve yoğunluğuna bağlı olarak beyin bunları işler ve nöronun aktive edilip edilmeyeceğine (\"ateşlenip ateşlenmeyeceğine\") karar verir.\n",
    "\n",
    "👉 Derin öğrenmede, Aktivasyon Fonksiyonunun rolü de budur - bu yüzden Yapay Sinir Ağında genellikle Transfer Fonksiyonu olarak adlandırılır.  \n",
    "\n",
    "👉 Aktivasyon Fonksiyonunun birincil rolü, düğümden gelen toplam ağırlıklı girdiyi bir sonraki gizli katmana veya çıktı olarak beslenecek bir çıktı değerine dönüştürmektir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg\" style=\"width: 600px;height: 400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Sinir Ağları Mimarisinin Unsurları</h2>\n",
    "</center>\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d242974bcba9f8c670e03e_Group%20806.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👉 Yukarıdaki resimde, birbirine bağlı nöronlardan oluşan bir sinir ağı görebilirsiniz. Her biri ağırlığı, önyargısı ve aktivasyon fonksiyonu ile karakterize edilir.\n",
    "\n",
    "<h3>👇 İşte bu ağın diğer unsurları</h3>\n",
    "\n",
    "💥 Girdi Katmanı(Input layer)\n",
    "\n",
    "* Giriş katmanı etki alanından ham girdi alır. Bu katmanda herhangi bir hesaplama yapılmaz. Buradaki düğümler sadece bilgileri (özellikleri) gizli katmana aktarır.\n",
    "\n",
    "💥 Gizli Katman(Hidden layer)\n",
    "\n",
    "* Adından da anlaşılacağı gibi, bu katmanın düğümleri açıkta değildir. Sinir ağına bir soyutlama sağlarlar.\n",
    "\n",
    "* Gizli katman, giriş katmanından girilen özellikler üzerinde her türlü hesaplamayı yapar ve sonucu çıkış katmanına aktarır.\n",
    "\n",
    "💥 Çıktı Katmanı(Output layer)\n",
    "\n",
    "* Gizli katman aracılığıyla öğrenilen bilgileri bir araya getiren ve sonuç olarak nihai değeri veren ağın son katmanıdır.\n",
    "\n",
    "📢 Not: Tüm gizli katmanlar genellikle **aynı aktivasyon** fonksiyonunu kullanır. Ancak, **çıktı katmanı tipik olarak gizli katmanlardan farklı bir aktivasyon fonksiyonu** kullanacaktır. Seçim, model tarafından yapılan tahminin amacına veya türüne bağlıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>İleri Besleme ve Geri Yayılım (Feedforward vs. Backpropagation)</h2>\n",
    "</center>\n",
    "\n",
    "✌️ Sinir ağları hakkında bilgi edinirken, bilginin hareketini tanımlayan iki temel terimle karşılaşırsınız: ileri ve geri yayılım.\n",
    "\n",
    "Hadi onları keşfedelim.\n",
    "\n",
    "💡 İleri Beslemeli Yayılım (Feedforward) - bilgi akışı ileri yönde gerçekleşir. Girdi, gizli katmandaki bazı ara işlevleri hesaplamak için kullanılır ve bu işlev daha sonra bir çıktıyı hesaplamak için kullanılır.\n",
    "\n",
    "👉 İleri beslemeli yayılımda Aktivasyon Fonksiyonu, mevcut nöronu besleyen girdi ile bir sonraki katmana giden çıktısı arasında yer alan matematiksel bir *kapıdır*.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24112aa6bf00ee6dc2642_Group%20805.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "💡 Geriye Yayılım (Backpropagation)- ağ bağlantılarının ağırlıkları, ağın gerçek çıktı vektörü ile istenen çıktı vektörü arasındaki farkı en aza indirmek için tekrar tekrar ayarlanır.\n",
    "\n",
    "👉 Basitçe söylemek gerekirse, geri yayılım, ağın ağırlıklarını ve önyargılarını ayarlayarak maliyet fonksiyonunu en aza indirmeyi amaçlar. Maliyet fonksiyonu gradyanları, aktivasyon fonksiyonu, ağırlıklar, önyargılar vb. gibi parametrelere göre ayarlama seviyesini belirler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Sinir Ağları Neden Aktivasyon Fonksiyonuna İhtiyaç Duyar?</h2>\n",
    "</center>\n",
    "\n",
    "👉 Aktivasyon fonksiyonunun amacı sinir ağına doğrusal olmayan bir özellik katmaktır.\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d243324bcba9144370e261_Screenshot%20(205).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👉 Aktivasyon fonksiyonları, ileri yayılma sırasında her katmanda ek bir adım getirir, ancak hesaplanması buna değer. İşte nedeni;\n",
    "\n",
    "* Aktivasyon fonksiyonları olmadan çalışan bir sinir ağımız olduğunu varsayalım.\n",
    "* Bu durumda, her nöron ağırlıkları ve önyargıları kullanarak girdiler üzerinde yalnızca doğrusal bir dönüşüm gerçekleştirecektir. Çünkü sinir ağına kaç tane gizli katman eklediğimiz önemli değildir; tüm katmanlar aynı şekilde davranacaktır çünkü iki doğrusal fonksiyonun bileşimi doğrusal bir fonksiyonun kendisidir.\n",
    "\n",
    "* Sinir ağı daha basit hale gelse de, herhangi bir karmaşık görevi öğrenmek imkansızdır ve modelimiz sadece doğrusal bir regresyon modeli olacaktır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>3 Tip Sinir Ağı Aktivasyon Fonksiyonu(3 Types of Neural Networks Activation Functions)</h2>\n",
    "</center>\n",
    "\n",
    "👇 Şimdi, temel kavramları ele aldığımıza göre, en popüler sinir ağları aktivasyon fonksiyonlarının üzerinden geçelim.\n",
    "\n",
    "1️⃣ İkili Adım Fonksiyonu (Binary Step Function)\n",
    "* İkili adım fonksiyonu, bir nöronun etkinleştirilip etkinleştirilmeyeceğine karar veren bir eşik değerine bağlıdır.\n",
    "\n",
    "* Aktivasyon fonksiyonuna beslenen girdi belirli bir eşikle karşılaştırılır; girdi bu eşikten büyükse nöron etkinleştirilir, aksi takdirde devre dışı bırakılır, yani çıktısı bir sonraki gizli katmana aktarılmaz.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2449a8f32de661dfd2c8b_pasted%20image%200%20(3).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be0e9ab5cab603f27e8f4a_math-20210607%20(3).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "👇 İkili adım fonksiyonunun bazı sınırlamaları şunlardır:\n",
    "\n",
    "* Çok değerli çıktılar sağlayamaz; örneğin, çok sınıflı sınıflandırma problemleri için kullanılamaz.\n",
    "* Adım fonksiyonunun gradyanı sıfırdır, bu da geriye yayılma sürecinde bir engele neden olur.\n",
    "\n",
    "2️⃣ Doğrusal Aktivasyon Fonksiyonu (Linear Activation Function)\n",
    "* *Aktivasyon yok* veya *özdeşlik fonksiyonu* (x1.0 ile çarpılır) olarak da bilinen doğrusal aktivasyon fonksiyonu, aktivasyonun girişle orantılı olduğu durumdur.\n",
    "\n",
    "* Fonksiyon, girdinin ağırlıklı toplamına hiçbir şey yapmaz, sadece kendisine verilen değeri çıkarır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d244bb0e12c94fb442c01e_pasted%20image%200%20(4).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2b655d7486422e5de081_math-20210607%20(20).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "✌️ Bununla birlikte, doğrusal bir aktivasyon fonksiyonunun iki önemli sorunu vardır:\n",
    "\n",
    "* Fonksiyonun türevi bir sabit olduğundan ve giriş x ile bir ilişkisi olmadığından geriye yayılımı kullanmak mümkün değildir.\n",
    "* Doğrusal bir aktivasyon fonksiyonu kullanılırsa sinir ağının tüm katmanları tek bir katmana çökecektir. Sinir ağındaki katman sayısı ne olursa olsun, son katman yine de ilk katmanın doğrusal bir fonksiyonu olacaktır. Yani, esasen, doğrusal bir aktivasyon fonksiyonu sinir ağını sadece tek bir katmana dönüştürür.\n",
    "\n",
    "3️⃣ Doğrusal Olmayan Aktivasyon Fonksiyonları(Non-Linear Activation Functions)\n",
    "\n",
    "* Yukarıda gösterilen doğrusal aktivasyon fonksiyonu basitçe bir doğrusal regresyon modelidir.\n",
    "* Sınırlı gücü nedeniyle bu, modelin ağın girdileri ve çıktıları arasında karmaşık eşlemeler oluşturmasına izin vermez.\n",
    "👇 Doğrusal olmayan aktivasyon fonksiyonları, doğrusal aktivasyon fonksiyonlarının aşağıdaki sınırlamalarını çözer:\n",
    "\n",
    "* Geriye yayılmaya izin verirler çünkü artık türev fonksiyonu girdiyle ilişkili olacaktır ve geriye dönüp girdi nöronlarındaki hangi ağırlıkların daha iyi bir tahmin sağlayabileceğini anlamak mümkündür.\n",
    "* Çıktı artık birden fazla katmandan geçen girdinin doğrusal olmayan bir kombinasyonu olacağından, birden fazla nöron katmanının istiflenmesine izin verirler. Herhangi bir çıktı, bir sinir ağında işlevsel bir hesaplama olarak temsil edilebilir.\n",
    "\n",
    "🤔 Şimdi, on farklı doğrusal olmayan sinir ağı aktivasyon fonksiyonuna ve bunların özelliklerine bir göz atalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<h2>10 Doğrusal Olmayan Sinir Ağları Aktivasyon Fonksiyonları(10 Non-Linear Neural Networks Activation Functions)</h2>\n",
    "</center>\n",
    "\n",
    "1️⃣ Sigmoid / Lojistik Aktivasyon Fonksiyonu (Sigmoid / Logistic Activation Function )\n",
    "\n",
    "* Bu fonksiyon herhangi bir gerçek değeri girdi olarak alır ve 0 ila 1 aralığındaki değerleri çıktı olarak verir.\n",
    "* Giriş ne kadar büyük olursa (daha pozitif), çıkış değeri 1,0'a o kadar yakın olurken, giriş ne kadar küçük olursa (daha negatif), çıkış aşağıda gösterildiği gibi 0,0'a o kadar yakın olacaktır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24547f85f71e3bd2339f8_pasted%20image%200%20(5).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be196326da6e49a7a15661_math-20210607%20(7).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "✌️ İşte sigmoid/logistik aktivasyon fonksiyonunun en yaygın kullanılan fonksiyonlardan biri olmasının nedeni:\n",
    "\n",
    "* Genellikle olasılığı bir çıktı olarak tahmin etmemiz gereken modeller için kullanılır. Herhangi bir şeyin olasılığı yalnızca 0 ve 1 aralığında mevcut olduğundan, sigmoid aralığı nedeniyle doğru seçimdir.\n",
    "* Fonksiyon türevlenebilir ve düzgün bir gradyan sağlar, yani çıktı değerlerinde sıçramaları önler. Bu, sigmoid aktivasyon fonksiyonunun S şekli ile temsil edilir.\n",
    "\n",
    "👇 Sigmoid fonksiyonun sınırlamaları aşağıda tartışılmaktadır:\n",
    "\n",
    "* Fonksiyonun türevi f'(x) = sigmoid(x)*(1-sigmoid(x)) şeklindedir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2458396cfc63d0a02b0c8_pasted%20image%200%20(6).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "☝️ Yukarıdaki Şekilden de görebileceğimiz gibi, gradyan değerleri yalnızca -3 ila 3 aralığında anlamlıdır ve grafik diğer bölgelerde çok daha düz hale gelir.\n",
    "\n",
    "* Bu, 3'ten büyük veya -3'ten küçük değerler için fonksiyonun çok küçük gradyanlara sahip olacağı anlamına gelir. Gradyan değeri sıfıra yaklaştıkça, ağ öğrenmeyi bırakır ve Kaybolan gradyan sorunundan muzdarip olur.\n",
    "\n",
    "* Lojistik fonksiyonun çıktısı sıfır etrafında simetrik değildir. Dolayısıyla tüm nöronların çıktıları aynı işarette olacaktır. Bu da sinir ağının eğitimini daha zor ve kararsız hale getirir.\n",
    "\n",
    "2️⃣ Tanh Fonksiyonu (Hiperbolik Tanjant) (Tanh Function (Hyperbolic Tangent))\n",
    "\n",
    "* Tanh fonksiyonu sigmoid/logistik aktivasyon fonksiyonuna çok benzer ve hatta -1 ila 1 çıkış aralığı farkıyla aynı S şekline sahiptir. Tanh'da, giriş ne kadar büyükse (daha pozitif), çıkış değeri 1.0'a o kadar yakın olurken, giriş ne kadar küçükse (daha negatif), çıkış -1.0'a o kadar yakın olacaktır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d246555e0bd43f4bf17b77_Group%2022.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1a9bac9b73842964585e_math-20210607%20(8).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "✌️ Bu aktivasyon fonksiyonunu kullanmanın avantajları şunlardır:\n",
    "\n",
    "* Tanh aktivasyon fonksiyonunun çıkışı Sıfır merkezlidir; dolayısıyla çıkış değerlerini kolayca güçlü negatif, nötr veya güçlü pozitif olarak eşleyebiliriz.\n",
    "* Genellikle bir sinir ağının gizli katmanlarında kullanılır, çünkü değerleri -1 ile -1 arasında yer alır; bu nedenle gizli katmanın ortalaması 0 veya ona çok yakın çıkar. Verilerin merkezlenmesine yardımcı olur ve bir sonraki katman için öğrenmeyi çok daha kolay hale getirir.\n",
    "Sınırlamalarını anlamak için tanh aktivasyon fonksiyonunun gradyanına bir göz atın.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d245b38d468ca69828e8f5_pasted%20image%200%20(7).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Gördüğünüz gibi sigmoid aktivasyon fonksiyonuna benzer şekilde kaybolan gradyan sorunuyla da karşı karşıyadır. Ayrıca tanh fonksiyonunun gradyanı sigmoid fonksiyona kıyasla çok daha diktir.\n",
    "\n",
    "💡 Not: Hem sigmoid hem de tanh kaybolan gradyan sorunuyla karşılaşsa da, tanh sıfır merkezlidir ve gradyanlar belirli bir yönde hareket etmekle kısıtlı değildir. Bu nedenle, pratikte tanh doğrusal olmayanlığı her zaman sigmoid doğrusal olmayanlığa tercih edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3️⃣ ReLU Fonksiyonu (ReLU Function)\n",
    "\n",
    "* ReLU, Doğrultulmuş Doğrusal Birim anlamına gelir.\n",
    "\n",
    "* Doğrusal bir fonksiyon izlenimi vermesine rağmen, ReLU bir türev fonksiyonuna sahiptir ve geriye yayılmaya izin verirken aynı zamanda hesaplama açısından verimli hale getirir.\n",
    "\n",
    "* Buradaki ana sorun, ReLU fonksiyonunun tüm nöronları aynı anda aktive etmemesidir.\n",
    "\n",
    "* Nöronlar yalnızca doğrusal dönüşümün çıktısı 0'dan küçükse devre dışı bırakılacaktır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24d1ac2cc1ded69730feb_relu.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1b236a3731df9b1d43c9_math-20210607%20(11).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "✌️ Aktivasyon fonksiyonu olarak ReLU kullanmanın avantajları aşağıdaki gibidir:\n",
    "\n",
    "* Yalnızca belirli sayıda nöron etkinleştirildiğinden, ReLU işlevi sigmoid ve tanh işlevlerine kıyasla hesaplama açısından çok daha verimlidir.\n",
    "* ReLU, doğrusal, doyurucu olmayan özelliği nedeniyle kayıp fonksiyonunun küresel minimumuna doğru gradyan inişinin yakınsamasını hızlandırır.\n",
    "Bu fonksiyonun karşılaştığı sınırlamalar şunlardır:\n",
    "\n",
    "👇 Aşağıda açıkladığım Dying ReLU sorunu: \n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2472c7c7683854f329e45_pasted%20image%200%20(9).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Grafiğin negatif tarafı gradyan değerini sıfır yapar. Bu nedenle, geriye yayılma işlemi sırasında bazı nöronların ağırlıkları ve önyargıları güncellenmez. Bu da hiç aktive olmayan ölü nöronlar yaratabilir.\n",
    "\n",
    "* Tüm negatif girdi değerleri hemen sıfır olur ve bu da modelin verilere düzgün bir şekilde uyma veya veriyi eğitme kabiliyetini azaltır.\n",
    "\n",
    "4️⃣ Sızdıran ReLU Fonksiyonu (Leaky ReLU Function)\n",
    "\n",
    "* Sızdıran ReLU, negatif alanda küçük bir pozitif eğime sahip olduğu için Ölen ReLU problemini çözmek için ReLU fonksiyonunun geliştirilmiş bir versiyonudur.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2474e3a0f7a4010b6129e_pasted%20image%200%20(10).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1b96b195d1119c30eddf_math-20210607%20(12).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* Sızdıran ReLU'nun avantajları ReLU ile aynıdır, buna ek olarak negatif giriş değerleri için bile geriye yayılımı mümkün kılar.\n",
    "\n",
    "* Negatif giriş değerleri için bu küçük değişiklik yapıldığında, grafiğin sol tarafındaki gradyan sıfır olmayan bir değer olarak ortaya çıkar. Dolayısıyla artık o bölgede ölü nöronlarla karşılaşmayacağız.\n",
    "\n",
    "👇 İşte Sızdıran ReLU fonksiyonunun türevi:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248619e91e2238803bb97_pasted%20image%200%20(11).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Bu işlevin karşılaştığı sınırlamalar şunlardır:\n",
    "\n",
    "* Tahminler negatif girdi değerleri için tutarlı olmayabilir.\n",
    "* Negatif değerler için gradyan küçük bir değerdir ve bu da model parametrelerinin öğrenilmesini zaman alıcı hale getirir.\n",
    "\n",
    "5️⃣ Parametrik ReLU Fonksiyonu (Parametric ReLU Function)\n",
    "\n",
    "* Parametrik ReLU, eksenin sol yarısı için gradyanın sıfır olması sorununu çözmeyi amaçlayan bir başka ReLU çeşididir.\n",
    "* Bu fonksiyon, fonksiyonun negatif kısmının eğimini a argümanı olarak sağlar. Geriye yayılım gerçekleştirilerek a'nın en uygun değeri öğrenilir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24887a3d0cc7966aa0aa7_pasted%20image%200%20(12).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2bc6703d396502ae28b2_math-20210607%20(21).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* Burada \"a\" negatif değerler için eğim parametresidir.\n",
    "\n",
    "* Sızdıran ReLU fonksiyonu ölü nöron sorununu çözmede hala başarısız olduğunda ve ilgili bilgi bir sonraki katmana başarılı bir şekilde aktarılamadığında parametrelendirilmiş ReLU fonksiyonu kullanılır.\n",
    "\n",
    "* Bu fonksiyonun sınırlaması, eğim parametresi a'nın değerine bağlı olarak farklı problemler için farklı performans gösterebilmesidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6️⃣ Üstel Doğrusal Birimler (ELUs) Fonksiyonu (Exponential Linear Units (ELUs) Function)\n",
    "\n",
    "* Üstel Doğrusal Birim ya da kısaca ELU, fonksiyonun negatif kısmının eğimini değiştiren bir ReLU çeşididir.\n",
    "* ELU, düz bir çizgiye sahip Sızdıran ReLU ve Parametrik ReLU fonksiyonlarının aksine negatif değerleri tanımlamak için bir log eğrisi kullanır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248d0a17eb381ecce9489_pasted%20image%200%20(13).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1d44cdd90474e0430225_math-20210607%20(14).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "✌️ ELU, aşağıdaki avantajları nedeniyle f ReLU için güçlü bir alternatiftir:\n",
    "\n",
    "* ELU, çıkışı -α'ya eşit olana kadar yavaşça düzgünleşirken RELU keskin bir şekilde düzgünleşir.\n",
    "* Girdinin negatif değerleri için log eğrisi sunarak ölü ReLU problemini önler. Ağın ağırlıkları ve önyargıları doğru yönde dürtmesine yardımcı olur.\n",
    "\n",
    "👇 ELU fonksiyonunun sınırlamaları aşağıdaki gibidir:\n",
    "\n",
    "* İçerdiği üstel işlem nedeniyle hesaplama süresini artırır\n",
    "* 'a' değerinin öğrenilmesi gerçekleşmez\n",
    "* Patlayan gradyan problemi\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248fe0377d17681762682_pasted%20image%200%20(14).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2464de5b4b70afe7c94d_math-20210607%20(15).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7️⃣ Softmax Fonksiyonu (Softmax Fonksiyonu)\n",
    "\n",
    "* Softmax aktivasyon fonksiyonunun giriş ve çıkışlarını keşfetmeden önce, yapı taşı olan ve olasılık değerlerini hesaplamaya çalışan sigmoid/logistik aktivasyon fonksiyonuna odaklanmalıyız.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24a027c76831f9b32af00_probability.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Sigmoid fonksiyonunun çıktısı, olasılık olarak düşünülebilecek 0 ila 1 aralığındaydı.\n",
    "👇 Ama bu işlev bazı sorunlarla karşı karşıyadır.\n",
    "\n",
    "Elimizde sırasıyla 0,8, 0,9, 0,7, 0,8 ve 0,6 olmak üzere beş çıktı değeri olduğunu varsayalım. Bununla nasıl ilerleyebiliriz?\n",
    "\n",
    "Cevap şu: Yapamayız.\n",
    "\n",
    "* Tüm sınıfların/çıktı olasılıklarının toplamı 1'e eşit olması gerektiğinden yukarıdaki değerler mantıklı değildir.\n",
    "\n",
    "* Gördüğünüz gibi, Softmax fonksiyonu birden fazla sigmoidin bir kombinasyonu olarak tanımlanmaktadır.\n",
    "\n",
    "* Göreceli olasılıkları hesaplar. Sigmoid/logistik aktivasyon fonksiyonuna benzer şekilde, SoftMax fonksiyonu her bir sınıfın olasılığını döndürür.\n",
    "\n",
    "* En yaygın olarak çok sınıflı sınıflandırma durumunda sinir ağının son katmanı için bir aktivasyon fonksiyonu olarak kullanılır.\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24ae385bcde8513aeb8c4_pasted%20image%200%20(15).jpg\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "🤔 Gelin birlikte basit bir örneğin üzerinden geçelim.\n",
    "\n",
    "* Üç sınıfınız olduğunu varsayalım, bu da çıktı katmanında üç nöron olacağı anlamına gelir. Şimdi, nöronlardan elde ettiğiniz çıktının [1.8, 0.9, 0.68] olduğunu varsayalım.\n",
    "\n",
    "* Olasılıksal bir görünüm vermek için bu değerler üzerinde softmax fonksiyonunu uygulamak aşağıdaki sonuçla sonuçlanacaktır: [0.58, 0.23, 0.19].\n",
    "\n",
    "* Fonksiyon, en büyük olasılık indeksi için 1 döndürürken, diğer iki dizi indeksi için 0 döndürür. Burada, indeks 0'a tam ağırlık verilirken indeks 1 ve indeks 2'ye ağırlık verilmez. Böylece çıktı, üç nörondan 1. nörona (indeks 0) karşılık gelen sınıf olacaktır.\n",
    "\n",
    "Softmax aktivasyon fonksiyonunun çok sınıflı sınıflandırma problemlerinde işleri nasıl kolaylaştırdığını şimdi görebilirsiniz.\n",
    "\n",
    "8️⃣ Swish\n",
    "\n",
    "* Google'daki araştırmacılar tarafından geliştirilen kendinden kapılı bir aktivasyon fonksiyonudur.\n",
    "\n",
    "Swish, görüntü sınıflandırma, makine çevirisi vb. gibi çeşitli zorlu alanlara uygulanan derin ağlarda ReLU aktivasyon fonksiyonuyla tutarlı bir şekilde eşleşir veya daha iyi performans gösterir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24c9fa6b752098fd3f047_pasted%20image%200%20(16).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Bu fonksiyon aşağıda sınırlıdır ancak yukarıda sınırsızdır, yani X negatif sonsuza yaklaştıkça Y sabit bir değere yaklaşır ancak X sonsuza yaklaştıkça Y sonsuza yaklaşır.\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be24d08129ce48ae90858e_math-20210607%20(16).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "👇 İşte Swish aktivasyon fonksiyonunun ReLU'ya göre birkaç avantajı:\n",
    "\n",
    "* Swish yumuşak bir fonksiyondur, yani ReLU'nun x = 0 yakınında yaptığı gibi aniden yön değiştirmez. Bunun yerine, 0'dan <0 değerlerine doğru yumuşak bir şekilde bükülür ve sonra tekrar yukarı doğru çıkar.\n",
    "* ReLU aktivasyon fonksiyonunda küçük negatif değerler sıfırlanmıştır. Ancak, bu negatif değerler verilerin altında yatan örüntüleri yakalamak için hala önemli olabilir. Büyük negatif değerler, seyreklik nedeniyle sıfırlanır ve bu da bir kazan-kazan durumu oluşturur.\n",
    "* Swish fonksiyonunun monoton olmaması, girdi verilerinin ve öğrenilecek ağırlığın ifadesini geliştirir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9️⃣ Gauss Hata Doğrusal Birimi (Gaussian Error Linear Unit (GELU))\n",
    "\n",
    "* Gaussian Error Linear Unit (GELU) aktivasyon fonksiyonu BERT, ROBERTa, ALBERT ve diğer en iyi NLP modelleri ile uyumludur. Bu aktivasyon fonksiyonu, dropout, zoneout ve ReLU'ların özelliklerini birleştirerek motive edilmiştir.\n",
    "* ReLU ve dropout birlikte bir nöronun çıktısını verir. ReLU bunu girişi sıfır veya bir ile çarparak (giriş değerinin pozitif veya negatif olmasına bağlı olarak) deterministik olarak, dropout ise sıfır ile çarparak stokastik olarak yapar.\n",
    "* Zoneout adı verilen RNN düzenleyici, girdileri stokastik olarak bir ile çarpar.\n",
    "* Bu işlevi, girişi stokastik olarak belirlenen ve girişe bağlı olan sıfır ya da bir ile çarparak birleştiriyoruz. Nöron girdisini x ile çarpıyoruz\n",
    "* m ∼ Bernoulli(Φ(x)), burada Φ(x) = P(X ≤x), X ∼ N (0, 1) standart normal dağılımın kümülatif dağılım fonksiyonudur.\n",
    "* Bu dağılım, nöron girdileri özellikle Toplu Normalleştirme ile normal bir dağılım izleme eğiliminde olduğu için seçilmiştir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24ce0ff3fd3ff13b04ef7_pasted%20image%200%20(17).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be260e5e9ac0bc4d8beac9_math-20210607%20(17).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* GELU doğrusal olmayanlığı, ReLU ve ELU aktivasyonlarından daha iyidir ve bilgisayarla görme(Computer Vision), doğal dil işleme(NLP) ve konuşma tanıma(Speech Recognition) alanlarındaki tüm görevlerde performans iyileştirmeleri bulur.\n",
    "\n",
    "🔟 Ölçeklendirilmiş Üstel Doğrusal Birim (SELU) (Scaled Exponential Linear Unit (SELU))\n",
    "\n",
    "* SELU, kendi kendini normalleştiren ağlarda tanımlanmıştır ve her katmanın önceki katmanlardan gelen ortalama ve varyansı koruduğu anlamına gelen iç normalleştirme ile ilgilenir. SELU, ortalama ve varyansı ayarlayarak bu normalleştirmeyi sağlar.\n",
    "* SELU, ortalamayı kaydırmak için hem pozitif hem de negatif değerlere sahiptir; bu, negatif değerler üretemediği için ReLU aktivasyon fonksiyonu için imkansızdır.\n",
    "* Varyansı ayarlamak için gradyanlar kullanılabilir. Aktivasyon fonksiyonunu artırmak için gradyanı birden büyük olan bir bölgeye ihtiyaç vardır.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24f17bb1afa7e8caa01dd_Group%20808.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "👇 Matematiksel olarak şu şekilde gösterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be074e54b1dd80b565fee6_math-20210607.png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* SELU'nun önceden tanımlanmış alfa α ve lambda λ değerleri vardır.\n",
    "\n",
    "👇 İşte SELU'nun ReLU'ya göre en büyük avantajı:\n",
    "\n",
    "* Dahili normalleştirme harici normalleştirmeden daha hızlıdır, bu da ağın daha hızlı yakınsadığı anlamına gelir.\n",
    "* SELU nispeten daha yeni bir aktivasyon fonksiyonudur ve karşılaştırmalı olarak keşfedildiği CNN'ler ve RNN'ler gibi mimariler üzerinde daha fazla makaleye ihtiyaç vardır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Derin sinir ağlarını eğitmek neden zordur?</h2>\n",
    "</center>\n",
    "\n",
    "Derin sinir ağlarınızı eğitirken karşılaşabileceğiniz iki zorluk vardır.\n",
    "\n",
    "🤔 Şimdi bunları daha ayrıntılı olarak tartışalım.\n",
    "\n",
    "💥 Vanishing Gradients (Kaybolan Gradyanlar)\n",
    "* Sigmoid fonksiyonu gibi, bazı aktivasyon fonksiyonları geniş bir girdi alanını 0 ile 1 arasında küçük bir çıktı alanına sıkıştırır.\n",
    "\n",
    "* Bu nedenle, sigmoid fonksiyonun girişindeki büyük bir değişiklik çıkışta küçük bir değişikliğe neden olacaktır. Dolayısıyla, türev küçük olur. Bu aktivasyonları kullanan sadece birkaç katmanlı sığ ağlar için bu büyük bir sorun değildir.\n",
    "\n",
    "* Ancak, daha fazla katman kullanıldığında, eğitimin etkili bir şekilde çalışması için gradyanın çok küçük olmasına neden olabilir.\n",
    "\n",
    "💥 Exploding Gradients (Patlayan Gradyanlar)\n",
    "* Patlayan gradyanlar, önemli hata gradyanlarının biriktiği ve eğitim sırasında sinir ağı modeli ağırlıklarında çok büyük güncellemelere neden olduğu problemlerdir.\n",
    "\n",
    "* Patlayan gradyanlar olduğunda kararsız bir ağ ortaya çıkabilir ve öğrenme tamamlanamaz.\n",
    "\n",
    "* Ağırlıkların değerleri de taşacak kadar büyük olabilir ve NaN değerleri olarak adlandırılan bir şeyle sonuçlanabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Doğru Aktivasyon Fonksiyonu nasıl seçilir?</h2>\n",
    "</center>\n",
    "\n",
    "👉 Çıktı katmanınız için aktivasyon fonksiyonunuzu, çözmekte olduğunuz tahmin probleminin türüne, özellikle de tahmin edilen değişkenin türüne göre eşleştirmeniz gerekir.\n",
    "\n",
    "👇 İşte aklınızda tutmanız gerekenler.\n",
    "\n",
    "Genel bir kural olarak, ReLU aktivasyon fonksiyonunu kullanarak başlayabilir ve ReLU optimum sonuçlar sağlamazsa diğer aktivasyon fonksiyonlarına geçebilirsiniz.\n",
    "\n",
    "* ReLU aktivasyon fonksiyonu sadece gizli katmanlarda kullanılmalıdır.\n",
    "* Sigmoid/Logistic ve Tanh fonksiyonları, modeli eğitim sırasında sorunlara karşı daha hassas hale getirdiğinden (kaybolan gradyanlar nedeniyle) gizli katmanlarda kullanılmamalıdır.\n",
    "* Swish fonksiyonu derinliği 40 katmandan fazla olan sinir ağlarında kullanılır.\n",
    "\n",
    "👇 Son olarak, çözmekte olduğunuz tahmin probleminin türüne bağlı olarak çıktı katmanınız için aktivasyon fonksiyonunu seçmek için birkaç kural:\n",
    "\n",
    "1️⃣ Regresyon - Doğrusal Aktivasyon Fonksiyonu\n",
    "\n",
    "2️⃣ İkili Sınıflandırma-Sigmoid/Lojistik Aktivasyon Fonksiyonu\n",
    "\n",
    "3️⃣ Çok Sınıflı Sınıflandırma-Softmax\n",
    "\n",
    "4️⃣ Çok Etiketli Sınıflandırma-Sigmoid\n",
    "\n",
    "* Gizli katmanlarda kullanılan aktivasyon fonksiyonu tipik olarak sinir ağı mimarisinin türüne göre seçilir.\n",
    "\n",
    "5️⃣ Konvolüsyonel Sinir Ağı (CNN): ReLU aktivasyon fonksiyonu.\n",
    "\n",
    "6️⃣ Tekrarlayan Sinir Ağı: Tanh ve/veya Sigmoid aktivasyon fonksiyonu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/62b18a8dc83132e1a479b65d_neural-network-activation-function-cheat-sheet.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 Şimdi, bu eğitimde öğrendiğiniz her şeyin hızlı bir özetini yapalım:\n",
    "\n",
    "* Aktivasyon Fonksiyonları ağa doğrusal olmayan bir yapı kazandırmak için kullanılır.\n",
    "* Bir sinir ağı neredeyse her zaman tüm gizli katmanlarda aynı aktivasyon fonksiyonuna sahip olacaktır. Bu aktivasyon fonksiyonu farklılaştırılabilir olmalıdır, böylece ağın parametreleri geriye yayılımda öğrenilir.\n",
    "* ReLU, gizli katmanlar için en yaygın kullanılan aktivasyon fonksiyonudur.\n",
    "* Bir aktivasyon fonksiyonu seçerken, karşılaşabileceği sorunları göz önünde bulundurmalısınız: vanishing ve exploding gradients.\n",
    "* Çıktı katmanı ile ilgili olarak, tahminlerin beklenen değer aralığını her zaman göz önünde bulundurmalıyız. Herhangi bir sayısal değer olabiliyorsa (regresyon probleminde olduğu gibi) doğrusal aktivasyon fonksiyonunu veya ReLU'yu kullanabilirsiniz.\n",
    "* Sınıflandırma problemleri için Softmax veya Sigmoid fonksiyonunu kullanın.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
