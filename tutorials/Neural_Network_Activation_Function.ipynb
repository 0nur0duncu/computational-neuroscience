{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.v7labs.com/blog/neural-networks-activation-functions referans alÄ±nmÄ±ÅŸtÄ±r.\n",
    "<center>\n",
    "<h2>Sinir AÄŸÄ± Aktivasyon Fonksiyonu nedir?</h2>\n",
    "</center>\n",
    "* Aktivasyon Fonksiyonu, bir nÃ¶ronun aktive edilip edilmemesi gerektiÄŸine karar verir. \n",
    "* Bu, daha basit matematiksel iÅŸlemler kullanarak nÃ¶ronun aÄŸa girdisinin tahmin sÃ¼recinde Ã¶nemli olup olmadÄ±ÄŸÄ±na karar vereceÄŸi anlamÄ±na gelir.\n",
    "* Aktivasyon Fonksiyonunun rolÃ¼, bir dÃ¼ÄŸÃ¼me (veya bir katmana) beslenen bir dizi girdi deÄŸerinden Ã§Ä±ktÄ± elde etmektir.\n",
    "\n",
    "ğŸ¤” DÃ¼ÄŸÃ¼m tam olarak nedir?\n",
    "\n",
    "ğŸ‘‰ Sinir aÄŸÄ±nÄ± beynimizle karÅŸÄ±laÅŸtÄ±rÄ±rsak, bir dÃ¼ÄŸÃ¼m, bir dizi giriÅŸ sinyali - harici uyaran - alan bir nÃ¶ronun kopyasÄ±dÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d243db4b12f1c8f339e8cb_unnamed.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‰ Bu girdi sinyallerinin doÄŸasÄ±na ve yoÄŸunluÄŸuna baÄŸlÄ± olarak beyin bunlarÄ± iÅŸler ve nÃ¶ronun aktive edilip edilmeyeceÄŸine (\"ateÅŸlenip ateÅŸlenmeyeceÄŸine\") karar verir.\n",
    "\n",
    "ğŸ‘‰ Derin Ã¶ÄŸrenmede, Aktivasyon Fonksiyonunun rolÃ¼ de budur - bu yÃ¼zden Yapay Sinir AÄŸÄ±nda genellikle Transfer Fonksiyonu olarak adlandÄ±rÄ±lÄ±r.  \n",
    "\n",
    "ğŸ‘‰ Aktivasyon Fonksiyonunun birincil rolÃ¼, dÃ¼ÄŸÃ¼mden gelen toplam aÄŸÄ±rlÄ±klÄ± girdiyi bir sonraki gizli katmana veya Ã§Ä±ktÄ± olarak beslenecek bir Ã§Ä±ktÄ± deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼rmektir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2424009416f21db643e21_Group%20807.jpg\" style=\"width: 600px;height: 400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Sinir AÄŸlarÄ± Mimarisinin UnsurlarÄ±</h2>\n",
    "</center>\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d242974bcba9f8c670e03e_Group%20806.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‰ YukarÄ±daki resimde, birbirine baÄŸlÄ± nÃ¶ronlardan oluÅŸan bir sinir aÄŸÄ± gÃ¶rebilirsiniz. Her biri aÄŸÄ±rlÄ±ÄŸÄ±, Ã¶nyargÄ±sÄ± ve aktivasyon fonksiyonu ile karakterize edilir.\n",
    "\n",
    "<h3>ğŸ‘‡ Ä°ÅŸte bu aÄŸÄ±n diÄŸer unsurlarÄ±</h3>\n",
    "\n",
    "ğŸ’¥ Girdi KatmanÄ±(Input layer)\n",
    "\n",
    "* GiriÅŸ katmanÄ± etki alanÄ±ndan ham girdi alÄ±r. Bu katmanda herhangi bir hesaplama yapÄ±lmaz. Buradaki dÃ¼ÄŸÃ¼mler sadece bilgileri (Ã¶zellikleri) gizli katmana aktarÄ±r.\n",
    "\n",
    "ğŸ’¥ Gizli Katman(Hidden layer)\n",
    "\n",
    "* AdÄ±ndan da anlaÅŸÄ±lacaÄŸÄ± gibi, bu katmanÄ±n dÃ¼ÄŸÃ¼mleri aÃ§Ä±kta deÄŸildir. Sinir aÄŸÄ±na bir soyutlama saÄŸlarlar.\n",
    "\n",
    "* Gizli katman, giriÅŸ katmanÄ±ndan girilen Ã¶zellikler Ã¼zerinde her tÃ¼rlÃ¼ hesaplamayÄ± yapar ve sonucu Ã§Ä±kÄ±ÅŸ katmanÄ±na aktarÄ±r.\n",
    "\n",
    "ğŸ’¥ Ã‡Ä±ktÄ± KatmanÄ±(Output layer)\n",
    "\n",
    "* Gizli katman aracÄ±lÄ±ÄŸÄ±yla Ã¶ÄŸrenilen bilgileri bir araya getiren ve sonuÃ§ olarak nihai deÄŸeri veren aÄŸÄ±n son katmanÄ±dÄ±r.\n",
    "\n",
    "ğŸ“¢ Not: TÃ¼m gizli katmanlar genellikle **aynÄ± aktivasyon** fonksiyonunu kullanÄ±r. Ancak, **Ã§Ä±ktÄ± katmanÄ± tipik olarak gizli katmanlardan farklÄ± bir aktivasyon fonksiyonu** kullanacaktÄ±r. SeÃ§im, model tarafÄ±ndan yapÄ±lan tahminin amacÄ±na veya tÃ¼rÃ¼ne baÄŸlÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Ä°leri Besleme ve Geri YayÄ±lÄ±m (Feedforward vs. Backpropagation)</h2>\n",
    "</center>\n",
    "\n",
    "âœŒï¸ Sinir aÄŸlarÄ± hakkÄ±nda bilgi edinirken, bilginin hareketini tanÄ±mlayan iki temel terimle karÅŸÄ±laÅŸÄ±rsÄ±nÄ±z: ileri ve geri yayÄ±lÄ±m.\n",
    "\n",
    "Hadi onlarÄ± keÅŸfedelim.\n",
    "\n",
    "ğŸ’¡ Ä°leri Beslemeli YayÄ±lÄ±m (Feedforward) - bilgi akÄ±ÅŸÄ± ileri yÃ¶nde gerÃ§ekleÅŸir. Girdi, gizli katmandaki bazÄ± ara iÅŸlevleri hesaplamak iÃ§in kullanÄ±lÄ±r ve bu iÅŸlev daha sonra bir Ã§Ä±ktÄ±yÄ± hesaplamak iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "ğŸ‘‰ Ä°leri beslemeli yayÄ±lÄ±mda Aktivasyon Fonksiyonu, mevcut nÃ¶ronu besleyen girdi ile bir sonraki katmana giden Ã§Ä±ktÄ±sÄ± arasÄ±nda yer alan matematiksel bir *kapÄ±dÄ±r*.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24112aa6bf00ee6dc2642_Group%20805.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ’¡ Geriye YayÄ±lÄ±m (Backpropagation)- aÄŸ baÄŸlantÄ±larÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ±, aÄŸÄ±n gerÃ§ek Ã§Ä±ktÄ± vektÃ¶rÃ¼ ile istenen Ã§Ä±ktÄ± vektÃ¶rÃ¼ arasÄ±ndaki farkÄ± en aza indirmek iÃ§in tekrar tekrar ayarlanÄ±r.\n",
    "\n",
    "ğŸ‘‰ BasitÃ§e sÃ¶ylemek gerekirse, geri yayÄ±lÄ±m, aÄŸÄ±n aÄŸÄ±rlÄ±klarÄ±nÄ± ve Ã¶nyargÄ±larÄ±nÄ± ayarlayarak maliyet fonksiyonunu en aza indirmeyi amaÃ§lar. Maliyet fonksiyonu gradyanlarÄ±, aktivasyon fonksiyonu, aÄŸÄ±rlÄ±klar, Ã¶nyargÄ±lar vb. gibi parametrelere gÃ¶re ayarlama seviyesini belirler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Sinir AÄŸlarÄ± Neden Aktivasyon Fonksiyonuna Ä°htiyaÃ§ Duyar?</h2>\n",
    "</center>\n",
    "\n",
    "ğŸ‘‰ Aktivasyon fonksiyonunun amacÄ± sinir aÄŸÄ±na doÄŸrusal olmayan bir Ã¶zellik katmaktÄ±r.\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d243324bcba9144370e261_Screenshot%20(205).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‰ Aktivasyon fonksiyonlarÄ±, ileri yayÄ±lma sÄ±rasÄ±nda her katmanda ek bir adÄ±m getirir, ancak hesaplanmasÄ± buna deÄŸer. Ä°ÅŸte nedeni;\n",
    "\n",
    "* Aktivasyon fonksiyonlarÄ± olmadan Ã§alÄ±ÅŸan bir sinir aÄŸÄ±mÄ±z olduÄŸunu varsayalÄ±m.\n",
    "* Bu durumda, her nÃ¶ron aÄŸÄ±rlÄ±klarÄ± ve Ã¶nyargÄ±larÄ± kullanarak girdiler Ã¼zerinde yalnÄ±zca doÄŸrusal bir dÃ¶nÃ¼ÅŸÃ¼m gerÃ§ekleÅŸtirecektir. Ã‡Ã¼nkÃ¼ sinir aÄŸÄ±na kaÃ§ tane gizli katman eklediÄŸimiz Ã¶nemli deÄŸildir; tÃ¼m katmanlar aynÄ± ÅŸekilde davranacaktÄ±r Ã§Ã¼nkÃ¼ iki doÄŸrusal fonksiyonun bileÅŸimi doÄŸrusal bir fonksiyonun kendisidir.\n",
    "\n",
    "* Sinir aÄŸÄ± daha basit hale gelse de, herhangi bir karmaÅŸÄ±k gÃ¶revi Ã¶ÄŸrenmek imkansÄ±zdÄ±r ve modelimiz sadece doÄŸrusal bir regresyon modeli olacaktÄ±r.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>3 Tip Sinir AÄŸÄ± Aktivasyon Fonksiyonu(3 Types of Neural Networks Activation Functions)</h2>\n",
    "</center>\n",
    "\n",
    "ğŸ‘‡ Åimdi, temel kavramlarÄ± ele aldÄ±ÄŸÄ±mÄ±za gÃ¶re, en popÃ¼ler sinir aÄŸlarÄ± aktivasyon fonksiyonlarÄ±nÄ±n Ã¼zerinden geÃ§elim.\n",
    "\n",
    "1ï¸âƒ£ Ä°kili AdÄ±m Fonksiyonu (Binary Step Function)\n",
    "* Ä°kili adÄ±m fonksiyonu, bir nÃ¶ronun etkinleÅŸtirilip etkinleÅŸtirilmeyeceÄŸine karar veren bir eÅŸik deÄŸerine baÄŸlÄ±dÄ±r.\n",
    "\n",
    "* Aktivasyon fonksiyonuna beslenen girdi belirli bir eÅŸikle karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r; girdi bu eÅŸikten bÃ¼yÃ¼kse nÃ¶ron etkinleÅŸtirilir, aksi takdirde devre dÄ±ÅŸÄ± bÄ±rakÄ±lÄ±r, yani Ã§Ä±ktÄ±sÄ± bir sonraki gizli katmana aktarÄ±lmaz.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2449a8f32de661dfd2c8b_pasted%20image%200%20(3).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be0e9ab5cab603f27e8f4a_math-20210607%20(3).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "ğŸ‘‡ Ä°kili adÄ±m fonksiyonunun bazÄ± sÄ±nÄ±rlamalarÄ± ÅŸunlardÄ±r:\n",
    "\n",
    "* Ã‡ok deÄŸerli Ã§Ä±ktÄ±lar saÄŸlayamaz; Ã¶rneÄŸin, Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma problemleri iÃ§in kullanÄ±lamaz.\n",
    "* AdÄ±m fonksiyonunun gradyanÄ± sÄ±fÄ±rdÄ±r, bu da geriye yayÄ±lma sÃ¼recinde bir engele neden olur.\n",
    "\n",
    "2ï¸âƒ£ DoÄŸrusal Aktivasyon Fonksiyonu (Linear Activation Function)\n",
    "* *Aktivasyon yok* veya *Ã¶zdeÅŸlik fonksiyonu* (x1.0 ile Ã§arpÄ±lÄ±r) olarak da bilinen doÄŸrusal aktivasyon fonksiyonu, aktivasyonun giriÅŸle orantÄ±lÄ± olduÄŸu durumdur.\n",
    "\n",
    "* Fonksiyon, girdinin aÄŸÄ±rlÄ±klÄ± toplamÄ±na hiÃ§bir ÅŸey yapmaz, sadece kendisine verilen deÄŸeri Ã§Ä±karÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d244bb0e12c94fb442c01e_pasted%20image%200%20(4).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2b655d7486422e5de081_math-20210607%20(20).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "âœŒï¸ Bununla birlikte, doÄŸrusal bir aktivasyon fonksiyonunun iki Ã¶nemli sorunu vardÄ±r:\n",
    "\n",
    "* Fonksiyonun tÃ¼revi bir sabit olduÄŸundan ve giriÅŸ x ile bir iliÅŸkisi olmadÄ±ÄŸÄ±ndan geriye yayÄ±lÄ±mÄ± kullanmak mÃ¼mkÃ¼n deÄŸildir.\n",
    "* DoÄŸrusal bir aktivasyon fonksiyonu kullanÄ±lÄ±rsa sinir aÄŸÄ±nÄ±n tÃ¼m katmanlarÄ± tek bir katmana Ã§Ã¶kecektir. Sinir aÄŸÄ±ndaki katman sayÄ±sÄ± ne olursa olsun, son katman yine de ilk katmanÄ±n doÄŸrusal bir fonksiyonu olacaktÄ±r. Yani, esasen, doÄŸrusal bir aktivasyon fonksiyonu sinir aÄŸÄ±nÄ± sadece tek bir katmana dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "\n",
    "3ï¸âƒ£ DoÄŸrusal Olmayan Aktivasyon FonksiyonlarÄ±(Non-Linear Activation Functions)\n",
    "\n",
    "* YukarÄ±da gÃ¶sterilen doÄŸrusal aktivasyon fonksiyonu basitÃ§e bir doÄŸrusal regresyon modelidir.\n",
    "* SÄ±nÄ±rlÄ± gÃ¼cÃ¼ nedeniyle bu, modelin aÄŸÄ±n girdileri ve Ã§Ä±ktÄ±larÄ± arasÄ±nda karmaÅŸÄ±k eÅŸlemeler oluÅŸturmasÄ±na izin vermez.\n",
    "ğŸ‘‡ DoÄŸrusal olmayan aktivasyon fonksiyonlarÄ±, doÄŸrusal aktivasyon fonksiyonlarÄ±nÄ±n aÅŸaÄŸÄ±daki sÄ±nÄ±rlamalarÄ±nÄ± Ã§Ã¶zer:\n",
    "\n",
    "* Geriye yayÄ±lmaya izin verirler Ã§Ã¼nkÃ¼ artÄ±k tÃ¼rev fonksiyonu girdiyle iliÅŸkili olacaktÄ±r ve geriye dÃ¶nÃ¼p girdi nÃ¶ronlarÄ±ndaki hangi aÄŸÄ±rlÄ±klarÄ±n daha iyi bir tahmin saÄŸlayabileceÄŸini anlamak mÃ¼mkÃ¼ndÃ¼r.\n",
    "* Ã‡Ä±ktÄ± artÄ±k birden fazla katmandan geÃ§en girdinin doÄŸrusal olmayan bir kombinasyonu olacaÄŸÄ±ndan, birden fazla nÃ¶ron katmanÄ±nÄ±n istiflenmesine izin verirler. Herhangi bir Ã§Ä±ktÄ±, bir sinir aÄŸÄ±nda iÅŸlevsel bir hesaplama olarak temsil edilebilir.\n",
    "\n",
    "ğŸ¤” Åimdi, on farklÄ± doÄŸrusal olmayan sinir aÄŸÄ± aktivasyon fonksiyonuna ve bunlarÄ±n Ã¶zelliklerine bir gÃ¶z atalÄ±m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<h2>10 DoÄŸrusal Olmayan Sinir AÄŸlarÄ± Aktivasyon FonksiyonlarÄ±(10 Non-Linear Neural Networks Activation Functions)</h2>\n",
    "</center>\n",
    "\n",
    "1ï¸âƒ£ Sigmoid / Lojistik Aktivasyon Fonksiyonu (Sigmoid / Logistic Activation Function )\n",
    "\n",
    "* Bu fonksiyon herhangi bir gerÃ§ek deÄŸeri girdi olarak alÄ±r ve 0 ila 1 aralÄ±ÄŸÄ±ndaki deÄŸerleri Ã§Ä±ktÄ± olarak verir.\n",
    "* GiriÅŸ ne kadar bÃ¼yÃ¼k olursa (daha pozitif), Ã§Ä±kÄ±ÅŸ deÄŸeri 1,0'a o kadar yakÄ±n olurken, giriÅŸ ne kadar kÃ¼Ã§Ã¼k olursa (daha negatif), Ã§Ä±kÄ±ÅŸ aÅŸaÄŸÄ±da gÃ¶sterildiÄŸi gibi 0,0'a o kadar yakÄ±n olacaktÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24547f85f71e3bd2339f8_pasted%20image%200%20(5).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be196326da6e49a7a15661_math-20210607%20(7).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "âœŒï¸ Ä°ÅŸte sigmoid/logistik aktivasyon fonksiyonunun en yaygÄ±n kullanÄ±lan fonksiyonlardan biri olmasÄ±nÄ±n nedeni:\n",
    "\n",
    "* Genellikle olasÄ±lÄ±ÄŸÄ± bir Ã§Ä±ktÄ± olarak tahmin etmemiz gereken modeller iÃ§in kullanÄ±lÄ±r. Herhangi bir ÅŸeyin olasÄ±lÄ±ÄŸÄ± yalnÄ±zca 0 ve 1 aralÄ±ÄŸÄ±nda mevcut olduÄŸundan, sigmoid aralÄ±ÄŸÄ± nedeniyle doÄŸru seÃ§imdir.\n",
    "* Fonksiyon tÃ¼revlenebilir ve dÃ¼zgÃ¼n bir gradyan saÄŸlar, yani Ã§Ä±ktÄ± deÄŸerlerinde sÄ±Ã§ramalarÄ± Ã¶nler. Bu, sigmoid aktivasyon fonksiyonunun S ÅŸekli ile temsil edilir.\n",
    "\n",
    "ğŸ‘‡ Sigmoid fonksiyonun sÄ±nÄ±rlamalarÄ± aÅŸaÄŸÄ±da tartÄ±ÅŸÄ±lmaktadÄ±r:\n",
    "\n",
    "* Fonksiyonun tÃ¼revi f'(x) = sigmoid(x)*(1-sigmoid(x)) ÅŸeklindedir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2458396cfc63d0a02b0c8_pasted%20image%200%20(6).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "â˜ï¸ YukarÄ±daki Åekilden de gÃ¶rebileceÄŸimiz gibi, gradyan deÄŸerleri yalnÄ±zca -3 ila 3 aralÄ±ÄŸÄ±nda anlamlÄ±dÄ±r ve grafik diÄŸer bÃ¶lgelerde Ã§ok daha dÃ¼z hale gelir.\n",
    "\n",
    "* Bu, 3'ten bÃ¼yÃ¼k veya -3'ten kÃ¼Ã§Ã¼k deÄŸerler iÃ§in fonksiyonun Ã§ok kÃ¼Ã§Ã¼k gradyanlara sahip olacaÄŸÄ± anlamÄ±na gelir. Gradyan deÄŸeri sÄ±fÄ±ra yaklaÅŸtÄ±kÃ§a, aÄŸ Ã¶ÄŸrenmeyi bÄ±rakÄ±r ve Kaybolan gradyan sorunundan muzdarip olur.\n",
    "\n",
    "* Lojistik fonksiyonun Ã§Ä±ktÄ±sÄ± sÄ±fÄ±r etrafÄ±nda simetrik deÄŸildir. DolayÄ±sÄ±yla tÃ¼m nÃ¶ronlarÄ±n Ã§Ä±ktÄ±larÄ± aynÄ± iÅŸarette olacaktÄ±r. Bu da sinir aÄŸÄ±nÄ±n eÄŸitimini daha zor ve kararsÄ±z hale getirir.\n",
    "\n",
    "2ï¸âƒ£ Tanh Fonksiyonu (Hiperbolik Tanjant) (Tanh Function (Hyperbolic Tangent))\n",
    "\n",
    "* Tanh fonksiyonu sigmoid/logistik aktivasyon fonksiyonuna Ã§ok benzer ve hatta -1 ila 1 Ã§Ä±kÄ±ÅŸ aralÄ±ÄŸÄ± farkÄ±yla aynÄ± S ÅŸekline sahiptir. Tanh'da, giriÅŸ ne kadar bÃ¼yÃ¼kse (daha pozitif), Ã§Ä±kÄ±ÅŸ deÄŸeri 1.0'a o kadar yakÄ±n olurken, giriÅŸ ne kadar kÃ¼Ã§Ã¼kse (daha negatif), Ã§Ä±kÄ±ÅŸ -1.0'a o kadar yakÄ±n olacaktÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d246555e0bd43f4bf17b77_Group%2022.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1a9bac9b73842964585e_math-20210607%20(8).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "âœŒï¸ Bu aktivasyon fonksiyonunu kullanmanÄ±n avantajlarÄ± ÅŸunlardÄ±r:\n",
    "\n",
    "* Tanh aktivasyon fonksiyonunun Ã§Ä±kÄ±ÅŸÄ± SÄ±fÄ±r merkezlidir; dolayÄ±sÄ±yla Ã§Ä±kÄ±ÅŸ deÄŸerlerini kolayca gÃ¼Ã§lÃ¼ negatif, nÃ¶tr veya gÃ¼Ã§lÃ¼ pozitif olarak eÅŸleyebiliriz.\n",
    "* Genellikle bir sinir aÄŸÄ±nÄ±n gizli katmanlarÄ±nda kullanÄ±lÄ±r, Ã§Ã¼nkÃ¼ deÄŸerleri -1 ile -1 arasÄ±nda yer alÄ±r; bu nedenle gizli katmanÄ±n ortalamasÄ± 0 veya ona Ã§ok yakÄ±n Ã§Ä±kar. Verilerin merkezlenmesine yardÄ±mcÄ± olur ve bir sonraki katman iÃ§in Ã¶ÄŸrenmeyi Ã§ok daha kolay hale getirir.\n",
    "SÄ±nÄ±rlamalarÄ±nÄ± anlamak iÃ§in tanh aktivasyon fonksiyonunun gradyanÄ±na bir gÃ¶z atÄ±n.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d245b38d468ca69828e8f5_pasted%20image%200%20(7).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi sigmoid aktivasyon fonksiyonuna benzer ÅŸekilde kaybolan gradyan sorunuyla da karÅŸÄ± karÅŸÄ±yadÄ±r. AyrÄ±ca tanh fonksiyonunun gradyanÄ± sigmoid fonksiyona kÄ±yasla Ã§ok daha diktir.\n",
    "\n",
    "ğŸ’¡ Not: Hem sigmoid hem de tanh kaybolan gradyan sorunuyla karÅŸÄ±laÅŸsa da, tanh sÄ±fÄ±r merkezlidir ve gradyanlar belirli bir yÃ¶nde hareket etmekle kÄ±sÄ±tlÄ± deÄŸildir. Bu nedenle, pratikte tanh doÄŸrusal olmayanlÄ±ÄŸÄ± her zaman sigmoid doÄŸrusal olmayanlÄ±ÄŸa tercih edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ ReLU Fonksiyonu (ReLU Function)\n",
    "\n",
    "* ReLU, DoÄŸrultulmuÅŸ DoÄŸrusal Birim anlamÄ±na gelir.\n",
    "\n",
    "* DoÄŸrusal bir fonksiyon izlenimi vermesine raÄŸmen, ReLU bir tÃ¼rev fonksiyonuna sahiptir ve geriye yayÄ±lmaya izin verirken aynÄ± zamanda hesaplama aÃ§Ä±sÄ±ndan verimli hale getirir.\n",
    "\n",
    "* Buradaki ana sorun, ReLU fonksiyonunun tÃ¼m nÃ¶ronlarÄ± aynÄ± anda aktive etmemesidir.\n",
    "\n",
    "* NÃ¶ronlar yalnÄ±zca doÄŸrusal dÃ¶nÃ¼ÅŸÃ¼mÃ¼n Ã§Ä±ktÄ±sÄ± 0'dan kÃ¼Ã§Ã¼kse devre dÄ±ÅŸÄ± bÄ±rakÄ±lacaktÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24d1ac2cc1ded69730feb_relu.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1b236a3731df9b1d43c9_math-20210607%20(11).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "âœŒï¸ Aktivasyon fonksiyonu olarak ReLU kullanmanÄ±n avantajlarÄ± aÅŸaÄŸÄ±daki gibidir:\n",
    "\n",
    "* YalnÄ±zca belirli sayÄ±da nÃ¶ron etkinleÅŸtirildiÄŸinden, ReLU iÅŸlevi sigmoid ve tanh iÅŸlevlerine kÄ±yasla hesaplama aÃ§Ä±sÄ±ndan Ã§ok daha verimlidir.\n",
    "* ReLU, doÄŸrusal, doyurucu olmayan Ã¶zelliÄŸi nedeniyle kayÄ±p fonksiyonunun kÃ¼resel minimumuna doÄŸru gradyan iniÅŸinin yakÄ±nsamasÄ±nÄ± hÄ±zlandÄ±rÄ±r.\n",
    "Bu fonksiyonun karÅŸÄ±laÅŸtÄ±ÄŸÄ± sÄ±nÄ±rlamalar ÅŸunlardÄ±r:\n",
    "\n",
    "ğŸ‘‡ AÅŸaÄŸÄ±da aÃ§Ä±kladÄ±ÄŸÄ±m Dying ReLU sorunu: \n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2472c7c7683854f329e45_pasted%20image%200%20(9).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* GrafiÄŸin negatif tarafÄ± gradyan deÄŸerini sÄ±fÄ±r yapar. Bu nedenle, geriye yayÄ±lma iÅŸlemi sÄ±rasÄ±nda bazÄ± nÃ¶ronlarÄ±n aÄŸÄ±rlÄ±klarÄ± ve Ã¶nyargÄ±larÄ± gÃ¼ncellenmez. Bu da hiÃ§ aktive olmayan Ã¶lÃ¼ nÃ¶ronlar yaratabilir.\n",
    "\n",
    "* TÃ¼m negatif girdi deÄŸerleri hemen sÄ±fÄ±r olur ve bu da modelin verilere dÃ¼zgÃ¼n bir ÅŸekilde uyma veya veriyi eÄŸitme kabiliyetini azaltÄ±r.\n",
    "\n",
    "4ï¸âƒ£ SÄ±zdÄ±ran ReLU Fonksiyonu (Leaky ReLU Function)\n",
    "\n",
    "* SÄ±zdÄ±ran ReLU, negatif alanda kÃ¼Ã§Ã¼k bir pozitif eÄŸime sahip olduÄŸu iÃ§in Ã–len ReLU problemini Ã§Ã¶zmek iÃ§in ReLU fonksiyonunun geliÅŸtirilmiÅŸ bir versiyonudur.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d2474e3a0f7a4010b6129e_pasted%20image%200%20(10).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1b96b195d1119c30eddf_math-20210607%20(12).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* SÄ±zdÄ±ran ReLU'nun avantajlarÄ± ReLU ile aynÄ±dÄ±r, buna ek olarak negatif giriÅŸ deÄŸerleri iÃ§in bile geriye yayÄ±lÄ±mÄ± mÃ¼mkÃ¼n kÄ±lar.\n",
    "\n",
    "* Negatif giriÅŸ deÄŸerleri iÃ§in bu kÃ¼Ã§Ã¼k deÄŸiÅŸiklik yapÄ±ldÄ±ÄŸÄ±nda, grafiÄŸin sol tarafÄ±ndaki gradyan sÄ±fÄ±r olmayan bir deÄŸer olarak ortaya Ã§Ä±kar. DolayÄ±sÄ±yla artÄ±k o bÃ¶lgede Ã¶lÃ¼ nÃ¶ronlarla karÅŸÄ±laÅŸmayacaÄŸÄ±z.\n",
    "\n",
    "ğŸ‘‡ Ä°ÅŸte SÄ±zdÄ±ran ReLU fonksiyonunun tÃ¼revi:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248619e91e2238803bb97_pasted%20image%200%20(11).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Bu iÅŸlevin karÅŸÄ±laÅŸtÄ±ÄŸÄ± sÄ±nÄ±rlamalar ÅŸunlardÄ±r:\n",
    "\n",
    "* Tahminler negatif girdi deÄŸerleri iÃ§in tutarlÄ± olmayabilir.\n",
    "* Negatif deÄŸerler iÃ§in gradyan kÃ¼Ã§Ã¼k bir deÄŸerdir ve bu da model parametrelerinin Ã¶ÄŸrenilmesini zaman alÄ±cÄ± hale getirir.\n",
    "\n",
    "5ï¸âƒ£ Parametrik ReLU Fonksiyonu (Parametric ReLU Function)\n",
    "\n",
    "* Parametrik ReLU, eksenin sol yarÄ±sÄ± iÃ§in gradyanÄ±n sÄ±fÄ±r olmasÄ± sorununu Ã§Ã¶zmeyi amaÃ§layan bir baÅŸka ReLU Ã§eÅŸididir.\n",
    "* Bu fonksiyon, fonksiyonun negatif kÄ±smÄ±nÄ±n eÄŸimini a argÃ¼manÄ± olarak saÄŸlar. Geriye yayÄ±lÄ±m gerÃ§ekleÅŸtirilerek a'nÄ±n en uygun deÄŸeri Ã¶ÄŸrenilir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24887a3d0cc7966aa0aa7_pasted%20image%200%20(12).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2bc6703d396502ae28b2_math-20210607%20(21).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* Burada \"a\" negatif deÄŸerler iÃ§in eÄŸim parametresidir.\n",
    "\n",
    "* SÄ±zdÄ±ran ReLU fonksiyonu Ã¶lÃ¼ nÃ¶ron sorununu Ã§Ã¶zmede hala baÅŸarÄ±sÄ±z olduÄŸunda ve ilgili bilgi bir sonraki katmana baÅŸarÄ±lÄ± bir ÅŸekilde aktarÄ±lamadÄ±ÄŸÄ±nda parametrelendirilmiÅŸ ReLU fonksiyonu kullanÄ±lÄ±r.\n",
    "\n",
    "* Bu fonksiyonun sÄ±nÄ±rlamasÄ±, eÄŸim parametresi a'nÄ±n deÄŸerine baÄŸlÄ± olarak farklÄ± problemler iÃ§in farklÄ± performans gÃ¶sterebilmesidir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ï¸âƒ£ Ãœstel DoÄŸrusal Birimler (ELUs) Fonksiyonu (Exponential Linear Units (ELUs) Function)\n",
    "\n",
    "* Ãœstel DoÄŸrusal Birim ya da kÄ±saca ELU, fonksiyonun negatif kÄ±smÄ±nÄ±n eÄŸimini deÄŸiÅŸtiren bir ReLU Ã§eÅŸididir.\n",
    "* ELU, dÃ¼z bir Ã§izgiye sahip SÄ±zdÄ±ran ReLU ve Parametrik ReLU fonksiyonlarÄ±nÄ±n aksine negatif deÄŸerleri tanÄ±mlamak iÃ§in bir log eÄŸrisi kullanÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248d0a17eb381ecce9489_pasted%20image%200%20(13).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be1d44cdd90474e0430225_math-20210607%20(14).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "âœŒï¸ ELU, aÅŸaÄŸÄ±daki avantajlarÄ± nedeniyle f ReLU iÃ§in gÃ¼Ã§lÃ¼ bir alternatiftir:\n",
    "\n",
    "* ELU, Ã§Ä±kÄ±ÅŸÄ± -Î±'ya eÅŸit olana kadar yavaÅŸÃ§a dÃ¼zgÃ¼nleÅŸirken RELU keskin bir ÅŸekilde dÃ¼zgÃ¼nleÅŸir.\n",
    "* Girdinin negatif deÄŸerleri iÃ§in log eÄŸrisi sunarak Ã¶lÃ¼ ReLU problemini Ã¶nler. AÄŸÄ±n aÄŸÄ±rlÄ±klarÄ± ve Ã¶nyargÄ±larÄ± doÄŸru yÃ¶nde dÃ¼rtmesine yardÄ±mcÄ± olur.\n",
    "\n",
    "ğŸ‘‡ ELU fonksiyonunun sÄ±nÄ±rlamalarÄ± aÅŸaÄŸÄ±daki gibidir:\n",
    "\n",
    "* Ä°Ã§erdiÄŸi Ã¼stel iÅŸlem nedeniyle hesaplama sÃ¼resini artÄ±rÄ±r\n",
    "* 'a' deÄŸerinin Ã¶ÄŸrenilmesi gerÃ§ekleÅŸmez\n",
    "* Patlayan gradyan problemi\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d248fe0377d17681762682_pasted%20image%200%20(14).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be2464de5b4b70afe7c94d_math-20210607%20(15).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7ï¸âƒ£ Softmax Fonksiyonu (Softmax Fonksiyonu)\n",
    "\n",
    "* Softmax aktivasyon fonksiyonunun giriÅŸ ve Ã§Ä±kÄ±ÅŸlarÄ±nÄ± keÅŸfetmeden Ã¶nce, yapÄ± taÅŸÄ± olan ve olasÄ±lÄ±k deÄŸerlerini hesaplamaya Ã§alÄ±ÅŸan sigmoid/logistik aktivasyon fonksiyonuna odaklanmalÄ±yÄ±z.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24a027c76831f9b32af00_probability.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Sigmoid fonksiyonunun Ã§Ä±ktÄ±sÄ±, olasÄ±lÄ±k olarak dÃ¼ÅŸÃ¼nÃ¼lebilecek 0 ila 1 aralÄ±ÄŸÄ±ndaydÄ±.\n",
    "ğŸ‘‡ Ama bu iÅŸlev bazÄ± sorunlarla karÅŸÄ± karÅŸÄ±yadÄ±r.\n",
    "\n",
    "Elimizde sÄ±rasÄ±yla 0,8, 0,9, 0,7, 0,8 ve 0,6 olmak Ã¼zere beÅŸ Ã§Ä±ktÄ± deÄŸeri olduÄŸunu varsayalÄ±m. Bununla nasÄ±l ilerleyebiliriz?\n",
    "\n",
    "Cevap ÅŸu: YapamayÄ±z.\n",
    "\n",
    "* TÃ¼m sÄ±nÄ±flarÄ±n/Ã§Ä±ktÄ± olasÄ±lÄ±klarÄ±nÄ±n toplamÄ± 1'e eÅŸit olmasÄ± gerektiÄŸinden yukarÄ±daki deÄŸerler mantÄ±klÄ± deÄŸildir.\n",
    "\n",
    "* GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, Softmax fonksiyonu birden fazla sigmoidin bir kombinasyonu olarak tanÄ±mlanmaktadÄ±r.\n",
    "\n",
    "* GÃ¶receli olasÄ±lÄ±klarÄ± hesaplar. Sigmoid/logistik aktivasyon fonksiyonuna benzer ÅŸekilde, SoftMax fonksiyonu her bir sÄ±nÄ±fÄ±n olasÄ±lÄ±ÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.\n",
    "\n",
    "* En yaygÄ±n olarak Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma durumunda sinir aÄŸÄ±nÄ±n son katmanÄ± iÃ§in bir aktivasyon fonksiyonu olarak kullanÄ±lÄ±r.\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24ae385bcde8513aeb8c4_pasted%20image%200%20(15).jpg\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "ğŸ¤” Gelin birlikte basit bir Ã¶rneÄŸin Ã¼zerinden geÃ§elim.\n",
    "\n",
    "* ÃœÃ§ sÄ±nÄ±fÄ±nÄ±z olduÄŸunu varsayalÄ±m, bu da Ã§Ä±ktÄ± katmanÄ±nda Ã¼Ã§ nÃ¶ron olacaÄŸÄ± anlamÄ±na gelir. Åimdi, nÃ¶ronlardan elde ettiÄŸiniz Ã§Ä±ktÄ±nÄ±n [1.8, 0.9, 0.68] olduÄŸunu varsayalÄ±m.\n",
    "\n",
    "* OlasÄ±lÄ±ksal bir gÃ¶rÃ¼nÃ¼m vermek iÃ§in bu deÄŸerler Ã¼zerinde softmax fonksiyonunu uygulamak aÅŸaÄŸÄ±daki sonuÃ§la sonuÃ§lanacaktÄ±r: [0.58, 0.23, 0.19].\n",
    "\n",
    "* Fonksiyon, en bÃ¼yÃ¼k olasÄ±lÄ±k indeksi iÃ§in 1 dÃ¶ndÃ¼rÃ¼rken, diÄŸer iki dizi indeksi iÃ§in 0 dÃ¶ndÃ¼rÃ¼r. Burada, indeks 0'a tam aÄŸÄ±rlÄ±k verilirken indeks 1 ve indeks 2'ye aÄŸÄ±rlÄ±k verilmez. BÃ¶ylece Ã§Ä±ktÄ±, Ã¼Ã§ nÃ¶rondan 1. nÃ¶rona (indeks 0) karÅŸÄ±lÄ±k gelen sÄ±nÄ±f olacaktÄ±r.\n",
    "\n",
    "Softmax aktivasyon fonksiyonunun Ã§ok sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma problemlerinde iÅŸleri nasÄ±l kolaylaÅŸtÄ±rdÄ±ÄŸÄ±nÄ± ÅŸimdi gÃ¶rebilirsiniz.\n",
    "\n",
    "8ï¸âƒ£ Swish\n",
    "\n",
    "* Google'daki araÅŸtÄ±rmacÄ±lar tarafÄ±ndan geliÅŸtirilen kendinden kapÄ±lÄ± bir aktivasyon fonksiyonudur.\n",
    "\n",
    "Swish, gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma, makine Ã§evirisi vb. gibi Ã§eÅŸitli zorlu alanlara uygulanan derin aÄŸlarda ReLU aktivasyon fonksiyonuyla tutarlÄ± bir ÅŸekilde eÅŸleÅŸir veya daha iyi performans gÃ¶sterir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24c9fa6b752098fd3f047_pasted%20image%200%20(16).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "* Bu fonksiyon aÅŸaÄŸÄ±da sÄ±nÄ±rlÄ±dÄ±r ancak yukarÄ±da sÄ±nÄ±rsÄ±zdÄ±r, yani X negatif sonsuza yaklaÅŸtÄ±kÃ§a Y sabit bir deÄŸere yaklaÅŸÄ±r ancak X sonsuza yaklaÅŸtÄ±kÃ§a Y sonsuza yaklaÅŸÄ±r.\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be24d08129ce48ae90858e_math-20210607%20(16).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "ğŸ‘‡ Ä°ÅŸte Swish aktivasyon fonksiyonunun ReLU'ya gÃ¶re birkaÃ§ avantajÄ±:\n",
    "\n",
    "* Swish yumuÅŸak bir fonksiyondur, yani ReLU'nun x = 0 yakÄ±nÄ±nda yaptÄ±ÄŸÄ± gibi aniden yÃ¶n deÄŸiÅŸtirmez. Bunun yerine, 0'dan <0 deÄŸerlerine doÄŸru yumuÅŸak bir ÅŸekilde bÃ¼kÃ¼lÃ¼r ve sonra tekrar yukarÄ± doÄŸru Ã§Ä±kar.\n",
    "* ReLU aktivasyon fonksiyonunda kÃ¼Ã§Ã¼k negatif deÄŸerler sÄ±fÄ±rlanmÄ±ÅŸtÄ±r. Ancak, bu negatif deÄŸerler verilerin altÄ±nda yatan Ã¶rÃ¼ntÃ¼leri yakalamak iÃ§in hala Ã¶nemli olabilir. BÃ¼yÃ¼k negatif deÄŸerler, seyreklik nedeniyle sÄ±fÄ±rlanÄ±r ve bu da bir kazan-kazan durumu oluÅŸturur.\n",
    "* Swish fonksiyonunun monoton olmamasÄ±, girdi verilerinin ve Ã¶ÄŸrenilecek aÄŸÄ±rlÄ±ÄŸÄ±n ifadesini geliÅŸtirir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9ï¸âƒ£ Gauss Hata DoÄŸrusal Birimi (Gaussian Error Linear Unit (GELU))\n",
    "\n",
    "* Gaussian Error Linear Unit (GELU) aktivasyon fonksiyonu BERT, ROBERTa, ALBERT ve diÄŸer en iyi NLP modelleri ile uyumludur. Bu aktivasyon fonksiyonu, dropout, zoneout ve ReLU'larÄ±n Ã¶zelliklerini birleÅŸtirerek motive edilmiÅŸtir.\n",
    "* ReLU ve dropout birlikte bir nÃ¶ronun Ã§Ä±ktÄ±sÄ±nÄ± verir. ReLU bunu giriÅŸi sÄ±fÄ±r veya bir ile Ã§arparak (giriÅŸ deÄŸerinin pozitif veya negatif olmasÄ±na baÄŸlÄ± olarak) deterministik olarak, dropout ise sÄ±fÄ±r ile Ã§arparak stokastik olarak yapar.\n",
    "* Zoneout adÄ± verilen RNN dÃ¼zenleyici, girdileri stokastik olarak bir ile Ã§arpar.\n",
    "* Bu iÅŸlevi, giriÅŸi stokastik olarak belirlenen ve giriÅŸe baÄŸlÄ± olan sÄ±fÄ±r ya da bir ile Ã§arparak birleÅŸtiriyoruz. NÃ¶ron girdisini x ile Ã§arpÄ±yoruz\n",
    "* m âˆ¼ Bernoulli(Î¦(x)), burada Î¦(x) = P(X â‰¤x), X âˆ¼ N (0, 1) standart normal daÄŸÄ±lÄ±mÄ±n kÃ¼mÃ¼latif daÄŸÄ±lÄ±m fonksiyonudur.\n",
    "* Bu daÄŸÄ±lÄ±m, nÃ¶ron girdileri Ã¶zellikle Toplu NormalleÅŸtirme ile normal bir daÄŸÄ±lÄ±m izleme eÄŸiliminde olduÄŸu iÃ§in seÃ§ilmiÅŸtir.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24ce0ff3fd3ff13b04ef7_pasted%20image%200%20(17).jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be260e5e9ac0bc4d8beac9_math-20210607%20(17).png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* GELU doÄŸrusal olmayanlÄ±ÄŸÄ±, ReLU ve ELU aktivasyonlarÄ±ndan daha iyidir ve bilgisayarla gÃ¶rme(Computer Vision), doÄŸal dil iÅŸleme(NLP) ve konuÅŸma tanÄ±ma(Speech Recognition) alanlarÄ±ndaki tÃ¼m gÃ¶revlerde performans iyileÅŸtirmeleri bulur.\n",
    "\n",
    "ğŸ”Ÿ Ã–lÃ§eklendirilmiÅŸ Ãœstel DoÄŸrusal Birim (SELU) (Scaled Exponential Linear Unit (SELU))\n",
    "\n",
    "* SELU, kendi kendini normalleÅŸtiren aÄŸlarda tanÄ±mlanmÄ±ÅŸtÄ±r ve her katmanÄ±n Ã¶nceki katmanlardan gelen ortalama ve varyansÄ± koruduÄŸu anlamÄ±na gelen iÃ§ normalleÅŸtirme ile ilgilenir. SELU, ortalama ve varyansÄ± ayarlayarak bu normalleÅŸtirmeyi saÄŸlar.\n",
    "* SELU, ortalamayÄ± kaydÄ±rmak iÃ§in hem pozitif hem de negatif deÄŸerlere sahiptir; bu, negatif deÄŸerler Ã¼retemediÄŸi iÃ§in ReLU aktivasyon fonksiyonu iÃ§in imkansÄ±zdÄ±r.\n",
    "* VaryansÄ± ayarlamak iÃ§in gradyanlar kullanÄ±labilir. Aktivasyon fonksiyonunu artÄ±rmak iÃ§in gradyanÄ± birden bÃ¼yÃ¼k olan bir bÃ¶lgeye ihtiyaÃ§ vardÄ±r.\n",
    "\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d24f17bb1afa7e8caa01dd_Group%20808.jpg\" style=\"width: 600px;height: 400px\"/></center>\n",
    "\n",
    "ğŸ‘‡ Matematiksel olarak ÅŸu ÅŸekilde gÃ¶sterilebilir:\n",
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60be074e54b1dd80b565fee6_math-20210607.png\" style=\"width: 400px;height: 200px;background-color: white\"/></center>\n",
    "\n",
    "* SELU'nun Ã¶nceden tanÄ±mlanmÄ±ÅŸ alfa Î± ve lambda Î» deÄŸerleri vardÄ±r.\n",
    "\n",
    "ğŸ‘‡ Ä°ÅŸte SELU'nun ReLU'ya gÃ¶re en bÃ¼yÃ¼k avantajÄ±:\n",
    "\n",
    "* Dahili normalleÅŸtirme harici normalleÅŸtirmeden daha hÄ±zlÄ±dÄ±r, bu da aÄŸÄ±n daha hÄ±zlÄ± yakÄ±nsadÄ±ÄŸÄ± anlamÄ±na gelir.\n",
    "* SELU nispeten daha yeni bir aktivasyon fonksiyonudur ve karÅŸÄ±laÅŸtÄ±rmalÄ± olarak keÅŸfedildiÄŸi CNN'ler ve RNN'ler gibi mimariler Ã¼zerinde daha fazla makaleye ihtiyaÃ§ vardÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Derin sinir aÄŸlarÄ±nÄ± eÄŸitmek neden zordur?</h2>\n",
    "</center>\n",
    "\n",
    "Derin sinir aÄŸlarÄ±nÄ±zÄ± eÄŸitirken karÅŸÄ±laÅŸabileceÄŸiniz iki zorluk vardÄ±r.\n",
    "\n",
    "ğŸ¤” Åimdi bunlarÄ± daha ayrÄ±ntÄ±lÄ± olarak tartÄ±ÅŸalÄ±m.\n",
    "\n",
    "ğŸ’¥ Vanishing Gradients (Kaybolan Gradyanlar)\n",
    "* Sigmoid fonksiyonu gibi, bazÄ± aktivasyon fonksiyonlarÄ± geniÅŸ bir girdi alanÄ±nÄ± 0 ile 1 arasÄ±nda kÃ¼Ã§Ã¼k bir Ã§Ä±ktÄ± alanÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r.\n",
    "\n",
    "* Bu nedenle, sigmoid fonksiyonun giriÅŸindeki bÃ¼yÃ¼k bir deÄŸiÅŸiklik Ã§Ä±kÄ±ÅŸta kÃ¼Ã§Ã¼k bir deÄŸiÅŸikliÄŸe neden olacaktÄ±r. DolayÄ±sÄ±yla, tÃ¼rev kÃ¼Ã§Ã¼k olur. Bu aktivasyonlarÄ± kullanan sadece birkaÃ§ katmanlÄ± sÄ±ÄŸ aÄŸlar iÃ§in bu bÃ¼yÃ¼k bir sorun deÄŸildir.\n",
    "\n",
    "* Ancak, daha fazla katman kullanÄ±ldÄ±ÄŸÄ±nda, eÄŸitimin etkili bir ÅŸekilde Ã§alÄ±ÅŸmasÄ± iÃ§in gradyanÄ±n Ã§ok kÃ¼Ã§Ã¼k olmasÄ±na neden olabilir.\n",
    "\n",
    "ğŸ’¥ Exploding Gradients (Patlayan Gradyanlar)\n",
    "* Patlayan gradyanlar, Ã¶nemli hata gradyanlarÄ±nÄ±n biriktiÄŸi ve eÄŸitim sÄ±rasÄ±nda sinir aÄŸÄ± modeli aÄŸÄ±rlÄ±klarÄ±nda Ã§ok bÃ¼yÃ¼k gÃ¼ncellemelere neden olduÄŸu problemlerdir.\n",
    "\n",
    "* Patlayan gradyanlar olduÄŸunda kararsÄ±z bir aÄŸ ortaya Ã§Ä±kabilir ve Ã¶ÄŸrenme tamamlanamaz.\n",
    "\n",
    "* AÄŸÄ±rlÄ±klarÄ±n deÄŸerleri de taÅŸacak kadar bÃ¼yÃ¼k olabilir ve NaN deÄŸerleri olarak adlandÄ±rÄ±lan bir ÅŸeyle sonuÃ§lanabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>DoÄŸru Aktivasyon Fonksiyonu nasÄ±l seÃ§ilir?</h2>\n",
    "</center>\n",
    "\n",
    "ğŸ‘‰ Ã‡Ä±ktÄ± katmanÄ±nÄ±z iÃ§in aktivasyon fonksiyonunuzu, Ã§Ã¶zmekte olduÄŸunuz tahmin probleminin tÃ¼rÃ¼ne, Ã¶zellikle de tahmin edilen deÄŸiÅŸkenin tÃ¼rÃ¼ne gÃ¶re eÅŸleÅŸtirmeniz gerekir.\n",
    "\n",
    "ğŸ‘‡ Ä°ÅŸte aklÄ±nÄ±zda tutmanÄ±z gerekenler.\n",
    "\n",
    "Genel bir kural olarak, ReLU aktivasyon fonksiyonunu kullanarak baÅŸlayabilir ve ReLU optimum sonuÃ§lar saÄŸlamazsa diÄŸer aktivasyon fonksiyonlarÄ±na geÃ§ebilirsiniz.\n",
    "\n",
    "* ReLU aktivasyon fonksiyonu sadece gizli katmanlarda kullanÄ±lmalÄ±dÄ±r.\n",
    "* Sigmoid/Logistic ve Tanh fonksiyonlarÄ±, modeli eÄŸitim sÄ±rasÄ±nda sorunlara karÅŸÄ± daha hassas hale getirdiÄŸinden (kaybolan gradyanlar nedeniyle) gizli katmanlarda kullanÄ±lmamalÄ±dÄ±r.\n",
    "* Swish fonksiyonu derinliÄŸi 40 katmandan fazla olan sinir aÄŸlarÄ±nda kullanÄ±lÄ±r.\n",
    "\n",
    "ğŸ‘‡ Son olarak, Ã§Ã¶zmekte olduÄŸunuz tahmin probleminin tÃ¼rÃ¼ne baÄŸlÄ± olarak Ã§Ä±ktÄ± katmanÄ±nÄ±z iÃ§in aktivasyon fonksiyonunu seÃ§mek iÃ§in birkaÃ§ kural:\n",
    "\n",
    "1ï¸âƒ£ Regresyon - DoÄŸrusal Aktivasyon Fonksiyonu\n",
    "\n",
    "2ï¸âƒ£ Ä°kili SÄ±nÄ±flandÄ±rma-Sigmoid/Lojistik Aktivasyon Fonksiyonu\n",
    "\n",
    "3ï¸âƒ£ Ã‡ok SÄ±nÄ±flÄ± SÄ±nÄ±flandÄ±rma-Softmax\n",
    "\n",
    "4ï¸âƒ£ Ã‡ok Etiketli SÄ±nÄ±flandÄ±rma-Sigmoid\n",
    "\n",
    "* Gizli katmanlarda kullanÄ±lan aktivasyon fonksiyonu tipik olarak sinir aÄŸÄ± mimarisinin tÃ¼rÃ¼ne gÃ¶re seÃ§ilir.\n",
    "\n",
    "5ï¸âƒ£ KonvolÃ¼syonel Sinir AÄŸÄ± (CNN): ReLU aktivasyon fonksiyonu.\n",
    "\n",
    "6ï¸âƒ£ Tekrarlayan Sinir AÄŸÄ±: Tanh ve/veya Sigmoid aktivasyon fonksiyonu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/62b18a8dc83132e1a479b65d_neural-network-activation-function-cheat-sheet.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‡ Åimdi, bu eÄŸitimde Ã¶ÄŸrendiÄŸiniz her ÅŸeyin hÄ±zlÄ± bir Ã¶zetini yapalÄ±m:\n",
    "\n",
    "* Aktivasyon FonksiyonlarÄ± aÄŸa doÄŸrusal olmayan bir yapÄ± kazandÄ±rmak iÃ§in kullanÄ±lÄ±r.\n",
    "* Bir sinir aÄŸÄ± neredeyse her zaman tÃ¼m gizli katmanlarda aynÄ± aktivasyon fonksiyonuna sahip olacaktÄ±r. Bu aktivasyon fonksiyonu farklÄ±laÅŸtÄ±rÄ±labilir olmalÄ±dÄ±r, bÃ¶ylece aÄŸÄ±n parametreleri geriye yayÄ±lÄ±mda Ã¶ÄŸrenilir.\n",
    "* ReLU, gizli katmanlar iÃ§in en yaygÄ±n kullanÄ±lan aktivasyon fonksiyonudur.\n",
    "* Bir aktivasyon fonksiyonu seÃ§erken, karÅŸÄ±laÅŸabileceÄŸi sorunlarÄ± gÃ¶z Ã¶nÃ¼nde bulundurmalÄ±sÄ±nÄ±z: vanishing ve exploding gradients.\n",
    "* Ã‡Ä±ktÄ± katmanÄ± ile ilgili olarak, tahminlerin beklenen deÄŸer aralÄ±ÄŸÄ±nÄ± her zaman gÃ¶z Ã¶nÃ¼nde bulundurmalÄ±yÄ±z. Herhangi bir sayÄ±sal deÄŸer olabiliyorsa (regresyon probleminde olduÄŸu gibi) doÄŸrusal aktivasyon fonksiyonunu veya ReLU'yu kullanabilirsiniz.\n",
    "* SÄ±nÄ±flandÄ±rma problemleri iÃ§in Softmax veya Sigmoid fonksiyonunu kullanÄ±n.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
